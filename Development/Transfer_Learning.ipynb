{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqfNtpShHwoF"
   },
   "source": [
    "# 1. Loading the Data from Google Drive\n",
    "---\n",
    "### 1.1\n",
    "\n",
    "Mount the drive and extract the dataset from images separated into corresponding folders\n",
    "\n",
    "The mounting of the drive is only necessary if the notebook is run on Google Colab instead of locally\n",
    "\n",
    "The folder is divided into:\n",
    "1. Test Folder\n",
    "  * With Mask Folder\n",
    "  * Without Mask Folder\n",
    "2. Train Folder\n",
    "  * With Mask Folder\n",
    "  * Without Mask Folder\n",
    "2. Validation Folder\n",
    "  * With Mask Folder\n",
    "  * Without Mask Folder\n",
    "\n",
    "---\n",
    "### 1.2\n",
    "\n",
    "All required modules are also imported after \n",
    "\n",
    "These modules are:\n",
    "  * Numpy\n",
    "  * Matplotlib.pyplot\n",
    "  * Seaborn\n",
    "  * Tensorflow\n",
    "  * Keras \n",
    "  * Scikit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tut1ncsHHyAk",
    "outputId": "d908b29d-5cf5-494b-a759-90be0f0560ba"
   },
   "outputs": [],
   "source": [
    "# 1.1 \n",
    "# Mounting to drive\n",
    "# This step is only necessary if the notebook is run on Google Colab instead of locally\n",
    "\n",
    "from google.colab import drive \n",
    "\n",
    "drive.mount(\"/content/gdrive\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6tIBnw6IOgW"
   },
   "outputs": [],
   "source": [
    "# 1.2\n",
    "# Import needed modules\n",
    "\n",
    "# Basic packages needed for data analysis, visualization and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Mainly Tensorflow packages for data preprocessing\n",
    "from PIL import Image \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "# Mainly Tensorflow.keras layers and pre-trained Convolutional Neural Network (CNN) models needed to do Transfer Learning\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Sequential, regularizers, Model\n",
    "\n",
    "# Mainly functions to load from saved checkpoints\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Mainly Tensorflow modules that help to optimize and fine-tune the CNN models better\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "from math import ceil \n",
    "\n",
    "# Mainly metrics to assess the CNN's performance\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiwvJEAtJRiW"
   },
   "source": [
    "# 2. Preprocessing the data and preparing the VGG19 model\n",
    "\n",
    "___\n",
    "### 2.1 Prepare ImageDataGenerator\n",
    "Create an ImageDataGenerator instance for data augmentation\n",
    "\n",
    "Considering the rotation, width and height shift, brightness, shear, zoom and horizontal flipping to mimic the possible real-world data the model would face\n",
    "\n",
    "Diversifies the dataset to let the model be trained on a larger and more diverse set of images\n",
    "\n",
    "___\n",
    "\n",
    "### 2.2 Construct CNN Model Using the Pre-trained VGG19\n",
    "The model's architecture would be built upon the VGG19 model trained on the ImageNet classification task \n",
    "\n",
    "The activation function for the hidden layers would be ReLU and Sigmoid, instead of VGG19's usual SoftMax, would be applied to the output layer\n",
    "\n",
    "The dense layers would have dropout regularization and L2 weight regularization would be applied \n",
    "\n",
    "The optimizer used is Adam, which gives an adaptive learning rate, the loss measured is \"binary cross-entropy\", early stopping would be implemented (loss is monitored for early stopping) and reducing LR on plateau is used too. \n",
    "\n",
    "The model is as follows: (_Refer to model summary for more info_)\n",
    "  1. Input layer (following our image size)\n",
    "  2. VGG19 (consists of 5 convolutional blocks but only the bottom 4 are used)\n",
    "  3. Flatten for dense network\n",
    "  4. First Dense Block\n",
    "    * Dense \n",
    "    * Dropout\n",
    "  5. Second Dense Block\n",
    "    * Dense\n",
    "    * Dropout\n",
    "  6. Output layer\n",
    "\n",
    "___ \n",
    "\n",
    "### 2.3 Setting Trainable Layers\n",
    "Set the bottom layers to be untrainable for fine-tuning the pre-trained model\n",
    "\n",
    "___\n",
    "\n",
    "### 2.4 Compile Model & Prepare Callbacks\n",
    "Compile the model with the Adam optimizer, Binary Cross-Entropy loss and Accuracy as the measure of success\n",
    "\n",
    "The callback objects for early stopping, saving model checkpoints and reducing LR on plateau are prepared\n",
    "\n",
    "___\n",
    "\n",
    "### 2.5 Loading Previous Models \n",
    "Instead of fitting a model to the dataset from scratch, we can also load a model that was saved from a previous training and resume training from there\n",
    "\n",
    "This can be done either by loading from checkpoints (CKPT format) or a saved model (HDF5 format)\n",
    "\n",
    "If the whole model is loaded, there is no need to construct the whole model from scratch, but the trainable layers have to be set correctly after loading the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoQmBcCkJTJL"
   },
   "outputs": [],
   "source": [
    "# 2.1 \n",
    "# ImageDataGenerator is an iterator for data augmentation\n",
    "\n",
    "# As the images of the dataset are quite closely zoomed onto faces already, \n",
    "# the shift range is limited to only 0.1 to prevent the faces from going out of frame\n",
    "\n",
    "# The brightness is not altered too drastically as the model should ultimately be used in a well lit setting\n",
    "\n",
    "# Vertical flipping is not applied as we do not expect to see upside down faces in our use cases\n",
    "\n",
    "# Rescaling pixels to a value between 0.0 and 1.0 as a form of normalization \n",
    "# to increase training speeds, stability and comprehensiveness\n",
    "\n",
    "target_img_size = (224, 224)\n",
    "\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rotation_range=40, # Randomly rotates the image by up to 40 degrees\n",
    "    width_shift_range=0.1, # Displaces the image horizontally by up to 10% of the original image size\n",
    "    height_shift_range=0.1, # Displaces the image vertically by up to 10% of the original image size\n",
    "    brightness_range=[0.8, 1.2], # Alters brightness by a positive 20% or negative 20% \n",
    "    shear_range=0.3, # Shears by up to 30% in the counter-clockwise direction\n",
    "    zoom_range=0.2, # Randomly zooms in and out by up to 20%\n",
    "    horizontal_flip=True, # Randomly flips the image horizontally\n",
    "    rescale=1./255 # Rescales pixels to a float between 0 and 1\n",
    "    ) \n",
    "\n",
    "datagen_val = ImageDataGenerator(\n",
    "    rotation_range=40, # Same requirements repeated for validation dataset\n",
    "    width_shift_range=0.1, \n",
    "    height_shift_range=0.1, \n",
    "    brightness_range=[0.8, 1.2], \n",
    "    shear_range=0.3, \n",
    "    zoom_range=0.2, \n",
    "    horizontal_flip=True, \n",
    "    rescale=1./255\n",
    "    ) \n",
    "\n",
    "datagen_test = ImageDataGenerator(\n",
    "    rescale=1./255 # No augmentation involved for testing dataset, just rescaling as the CNN is trained on normalized pixels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrcQxs32gs9p",
    "outputId": "3970ddff-2021-4f4a-9da2-6a96f8b14861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 1s 0us/step\n",
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2.2 \n",
    "# TL Model with 2 dense layers added to the top\n",
    "\n",
    "# Load the pretrained VGG19 model for transfer learning, specifying the input image size\n",
    "temp_model = VGG19(weights='imagenet', include_top=False, input_shape=(target_img_size[0], target_img_size[1], 3)) \n",
    "\n",
    "# Checking the model \n",
    "temp_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J1EZ9xJdhqfv",
    "outputId": "4f3c6f15-5cc0-4352-aa0b-f53b1f3d8004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "=================================================================\n",
      "Total params: 10,585,152\n",
      "Trainable params: 10,585,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the wanted layers\n",
    "\n",
    "# Only layers below block4_pool would be used\n",
    "\n",
    "# The pre-trained model's bottom layers are mainly involved in extracting more general data from images \n",
    "# such as edges, texture, shape, etc., the top layers are more fine-tuned towards the true classfication task\n",
    "\n",
    "# As such, we should preserve the bottom layers and \n",
    "# allow for changes and fine-tuning in the top layers as well as the newly added dense layers\n",
    "\n",
    "# Use 4 convol blocks instead of 5 from the VGG19 model\n",
    "output_layer = 'block4_pool' \n",
    "int_model = Model(inputs=temp_model.input, outputs=temp_model.get_layer(output_layer).output)\n",
    "# Use Keras's Functional API to form a Model object with a top layer of 'block4_pool'\n",
    "\n",
    "# Checking the model\n",
    "int_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xNO-tFbi9gv",
    "outputId": "61877424-081d-41ec-bd39-788b07fb703c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              100353000 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 111,439,153\n",
      "Trainable params: 111,439,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Adding desired dense layers \n",
    "\n",
    "# Use Keras's Functional API to add the dense layers\n",
    "# Each layer is chained to the existing Model model in sequence\n",
    "temp_out = int_model.output\n",
    "temp_out = Flatten()(temp_out)\n",
    "temp_out = Dense(1000, kernel_regularizer=regularizers.L2(0.0001), activation='relu')(temp_out)\n",
    "temp_out = Dropout(0.2)(temp_out)\n",
    "temp_out = Dense(500, kernel_regularizer=regularizers.L2(0.0001), activation='relu')(temp_out)\n",
    "temp_out = Dropout(0.2)(temp_out)\n",
    "predictions = Dense(1, activation='sigmoid')(temp_out)\n",
    "\n",
    "# Forming final model\n",
    "\n",
    "# Use Keras's Functional API to form a Model object with the top layers we defined ourselves\n",
    "model = Model(inputs=int_model.input, outputs=predictions)\n",
    "\n",
    "# Checking the FINAL model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K04QpPVoxIy1",
    "outputId": "98dafa00-29bc-41ac-dd1b-4e07ac07dda9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              100353000 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 111,439,153\n",
      "Trainable params: 109,113,585\n",
      "Non-trainable params: 2,325,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2.3\n",
    "# Set the top 5 CONVOL layers to be trainable and the other bottom layers to be untrainable\n",
    "# so that whatever the pre-trained model has learnt is still preserved but\n",
    "# the model is fine-tuned to our classification task\n",
    "\n",
    "# It is a given that the dense layers are set to be trainable\n",
    "\n",
    "TRAINABLE_LAYERS = 5 # Number of trainable CONVOL layers\n",
    "idx = -(TRAINABLE_LAYERS) # Multiply the number by -1 to form the negative index\n",
    "\n",
    "# Set all layers to trainable first\n",
    "for layer in model.layers[:]:\n",
    "  layer.trainable = True\n",
    "\n",
    "# Set bottom layers to be untrainable\n",
    "for layer in int_model.layers[:idx]: # specifies which (CONVOL) layers are trainable\n",
    "  layer.trainable = False\n",
    "\n",
    "# Double-checking the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbEWXHnDfn31"
   },
   "outputs": [],
   "source": [
    "# 2.4\n",
    "\n",
    "# Compiling the model \n",
    "opt = Adam(learning_rate=1e-4) # Using the adam optimizer which allows for adapting LRs, initialized as 0.0001\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) \n",
    "# Binary cross-entropy used as there are 2 categories predicted by one output neuron\n",
    "# Accuracy used as measure of success\n",
    "\n",
    "# Prepare for early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "\n",
    "# Prepare for model checkpoint saving\n",
    "checkpoint_path = \"/content/gdrive/<path to checkpoint storage>/<filename>.ckpt\"\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Prepare for reduced LR when approaching a plateau \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad1xBE5l3Nu8",
    "outputId": "24b3bfb0-c69f-4da4-b16a-6a9b53d4ba07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1bd4e6bc10>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.5\n",
    "# Load from a previously saved state of a trained model\n",
    "\n",
    "# Load previously saved weights from CKPT file\n",
    "model.load_weights(\"/content/gdrive/<path to CKPT file>/<filename>.ckpt\")\n",
    "\n",
    "# Load previously saved model from HDF5 file\n",
    "# model = load_model('/content/gdrive/<path to HDF5 file>/<filename>.h5')\n",
    "# If the whole model is loaded, there is no need to construct the whole model from scratch\n",
    "\n",
    "#####\n",
    "# IF the model is loaded directly from the HDF5 file, \n",
    "# there may be a need to set up the trainable layers after loading the model\n",
    "\n",
    "# Set all layers to trainable first\n",
    "# for layer in model.layers[:]:\n",
    "  # layer.trainable = True\n",
    "\n",
    "# Set bottom layers to be untrainable\n",
    "# for layer in int_model.layers[:-5]: # specifies which layers are trainable\n",
    "  # layer.trainable = False\n",
    "\n",
    "# Model summary\n",
    "# model.summary()\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine-tuning the Model\n",
    "\n",
    "___\n",
    "\n",
    "### 3.1 Prepare Dataset\n",
    "\n",
    "Prepare the training, validation and testing dataset using the flow_from_directory() method of the ImageDataGenerator object\n",
    "\n",
    "The method reads the images and yields them in batches from the corresponding folders using the filepath, batch size and target image size provided\n",
    "\n",
    "Shuffle is meant to introduce more randomness and the seed was set to 1 as a group decision in order to standardize within the group\n",
    "\n",
    "The images yielded may be altered according to the earlier set parameters at random\n",
    "\n",
    "Initially, the dataset was loaded using load_img() and img_to_array() and the augmentor iterates from memory using the flow() method. \n",
    "\n",
    "In order to standardize the dataset used however, flow_from_directory() method is now used instead to iterate from storage instead. \n",
    "\n",
    "___\n",
    "\n",
    "### 3.2 Fine-tuning the CNN Model\n",
    "\n",
    "The model is fitted/fine-tuned to the dataset, using the training and validation ImageDataGenerator and the 3 callback objects prepared\n",
    "\n",
    "The results are appended to a list called model_metadata\n",
    "\n",
    "___\n",
    "\n",
    "### 3.3 Load Previous Model\n",
    "\n",
    "Instead of fitting a new model to the dataset, we can also load a model that was saved from a previous training\n",
    "\n",
    "This can be done either by loading from checkpoints (CKPT format) or a saved model (HDF5 format)\n",
    "\n",
    "If the whole model is loaded, there is no need to construct the whole model from scratch\n",
    "\n",
    "___\n",
    "\n",
    "### 3.4 Evaluation\n",
    "\n",
    "The model's performance after fitting is then evaluated using the testing ImageDataGenerator\n",
    "\n",
    "A confusion matrix and classification report is also plotted to observe the model's other metrics of performance\n",
    "\n",
    "___\n",
    "\n",
    "### 3.5 Viewing the Predictions\n",
    "\n",
    "Using matplotlib.pyplot, we can see the image of predicted images alongside the prediction and true label\n",
    "\n",
    "___\n",
    "\n",
    "### 3.6 Saving Model\n",
    "\n",
    "The model can be saved in its entirety into an HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ln72UZkFw_e6",
    "outputId": "8125cc33-2741-4e2c-db12-37e0bc79d4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3561 images belonging to 2 classes.\n",
      "Found 1017 images belonging to 2 classes.\n",
      "Found 509 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 3.1\n",
    "\n",
    "# Standard batch size of 32 for time trade-off\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Paths to corresponding dataset folders\n",
    "train_path = \"/content/gdrive/<path to dataset storage>/data/train\" # Training dataset\n",
    "val_path = \"/content/gdrive/<path to dataset storage>/data/val\" # Validation dataset\n",
    "test_path = \"/content/gdrive/<path to dataset storage>/data/test\" # Testing dataset\n",
    "\n",
    "# ImageDataGenerator can directly read the dataset from disk and create batches of images that are altered at random\n",
    "\n",
    "# Target image size is (224, 224), batch size is 32, class mode refers to the 2 classes predicted by the CNN model\n",
    "# Shuffle set to True indicates that the generator would select images to alter and yield randomly from a dataset that is random shuffled\n",
    "# Testing ImageDataGenerator has shuffle set to False so that the class labels used for the confusion matrix would be in the correct order\n",
    "\n",
    "train_gen = datagen_train.flow_from_directory(train_path, target_size=target_img_size, batch_size=BATCH_SIZE, class_mode=\"binary\", shuffle=True, seed=1)\n",
    "\n",
    "val_gen = datagen_val.flow_from_directory(val_path, target_size=target_img_size, batch_size=BATCH_SIZE, class_mode=\"binary\", shuffle=True, seed=1)\n",
    "\n",
    "test_gen = datagen_test.flow_from_directory(test_path, target_size=target_img_size, batch_size=BATCH_SIZE, class_mode=\"binary\", shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2yMVFZlyNo6",
    "outputId": "b5615b5f-94bd-43b3-e8ef-fb8d77f3b4c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "112/112 [==============================] - 4536s 40s/step - loss: 1.3898 - accuracy: 0.6138 - val_loss: 0.2404 - val_accuracy: 0.9115\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24043, saving model to /content/gdrive/MyDrive/.ipynb_checkpoints/tl_vgg19_1/cp_6.ckpt\n",
      "Epoch 2/15\n",
      "112/112 [==============================] - 4407s 39s/step - loss: 0.1722 - accuracy: 0.9358 - val_loss: 0.1657 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24043 to 0.16566, saving model to /content/gdrive/MyDrive/.ipynb_checkpoints/tl_vgg19_1/cp_6.ckpt\n",
      "Epoch 3/15\n",
      "112/112 [==============================] - 4438s 39s/step - loss: 0.1750 - accuracy: 0.9326 - val_loss: 0.0949 - val_accuracy: 0.9656\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16566 to 0.09489, saving model to /content/gdrive/MyDrive/.ipynb_checkpoints/tl_vgg19_1/cp_6.ckpt\n",
      "Epoch 4/15\n",
      "112/112 [==============================] - 4411s 39s/step - loss: 0.1104 - accuracy: 0.9666 - val_loss: 0.0968 - val_accuracy: 0.9666\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09489\n"
     ]
    }
   ],
   "source": [
    "# 3.2\n",
    "# Preparing the list to store details of the model's training performance\n",
    "model_metadata = []\n",
    "\n",
    "# Fitting the model\n",
    "model_metadata.append(\n",
    "    model.fit_generator(train_gen, \n",
    "              steps_per_epoch=ceil(3561/BATCH_SIZE),\n",
    "              epochs=15, \n",
    "              callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "              validation_data=val_gen,\n",
    "              validation_steps=ceil(1017/BATCH_SIZE),\n",
    "              verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3\n",
    "# Load from a previously saved state of a trained model\n",
    "\n",
    "# Load previously saved weights from CKPT file\n",
    "# model.load_weights(\"/content/gdrive/<path to CKPT file>/<filename>.ckpt\")\n",
    "\n",
    "# Load previously saved model from HDF5 file\n",
    "model = load_model('/content/gdrive/<path to HDF5 file>/<filename>.h5')\n",
    "# If the whole model is loaded, there is no need to construct the whole model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-BZFVeJxkfh",
    "outputId": "66baf5ab-2a34-4821-a218-c5dd2f5bf7d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 279s 17s/step - loss: 0.0422 - accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# 3.4\n",
    "# Observing the results\n",
    "\n",
    "# Use evaluate() method to evaluate performance metrics of accuracy and loss\n",
    "model_metadata.append(\n",
    "    model.evaluate(test_gen, verbose=1, return_dict=True)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the returned results of the evaluate() method\n",
    "model_metadata[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6aBHx5SnyFi1",
    "outputId": "5ea1404c-b8c4-4922-f321-3eeb5d0a98b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f90965d1940>"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAV2ElEQVR4nO3deXhV1bnH8e+bAFrCPI8iQ0ShWlQepNfW6nVkaNHaUrBFtNh4KyooTsCtA0r1UkDqo4KxylALlPugFbmIIg4FrViwFBmkIg4QQ8KgDGIhw3v/yBEPEJJDcsjK2fw+PuvhnLX32Xudx+Tl5d1rr23ujoiIVL200AMQETleKQCLiASiACwiEogCsIhIIArAIiKB1DjWJyjYtlHTLOQwJ3XqG3oIUg3lfrHWKnuMo4k5NZt0qPT5KkMZsIhIIMc8AxYRqVLFRaFHkDAFYBGJlqLC0CNImAKwiESKe3HoISRMAVhEoqVYAVhEJAxlwCIigeginIhIIMqARUTCcM2CEBEJRBfhREQCUQlCRCQQXYQTEQlEGbCISCC6CCciEoguwomIhOGuGrCISBiqAYuIBKIShIhIIMqARUQCKSoIPYKEKQCLSLSoBCEiEohKECIigSgDFhEJRAFYRCQMT6GLcGmhByAiklRenHgrg5m1NbPXzGytma0xs2Gx/nvNLMfMVsZa77jPjDSzDWa23swuLW+oyoBFJFqSV4IoBEa4+7tmVhdYYWaLYtsedvfx8TubWRdgANAVaAW8YmaneBn3RisDFpFoSVIG7O657v5u7PVuYB3QuoyP9ANmu/s+d/8I2AD0KOscCsAiEi3FxQk3M8sys+VxLau0Q5rZycCZwLJY141mtsrMnjazhrG+1sCmuI9tpuyArQAsIhFzFBmwu2e7e/e4ln3o4cysDjAXGO7uu4DJQEegG5ALTKjoUFUDFpFoKUzeguxmVpOS4Psnd38WwN3z4rY/CcyPvc0B2sZ9vE2s74iUAYtItCRvFoQBTwHr3H1iXH/LuN2uAFbHXs8DBpjZCWbWHsgE3inrHMqARSRakjcL4lxgEPCema2M9Y0CBppZN8CBj4HrAdx9jZnNAdZSMoNiaFkzIEABWESiJklrQbj7UsBK2bSgjM+MBcYmeg4FYBGJFt2KLCISiFZDExEJJImzII41BWARiRb30CNImAKwiESLasAiIoEoAIuIBKKLcCIigRSVee9DtaIALCLRohKEiEggCsAiIoGoBiwiEoYXax6wiEgYKkGIiASiWRAiIoEoAxYRCUQBOPXl5m1l1P3j2f755xjGT/r1YlD/yw/aZ+eu3fzmwYfZlJPLCbVqcf+oW8jscHKlzrt//35G3j+Btes/oEH9eowfM5LWLZvz1jvvMmnKVAoKCqlZswYjhg7hnLO7VepcUvVatW7BI1MepGnTJrg7z0yfwx+mPEODBvWZMnUCbU9qzaZPc7j+mlvZuXNX6OGmphRajEfPhDuCGunp3H7Tr5j3p2xmZj/M7Gfn8+FHnxy0z5Mz/sypmR15bsZkfvub23ho0pSEj5+Tm8c1N95xWP+z81+mXt06vDjnaQb97HImPv40AA0b1OPR/7mX5/44mbH/PYKRY8ZX7gtKEIWFhdz33+P4Qc8f0ufiAVxz3VWc0rkjN95yHUvfeJtzz+7F0jfe5sZbrgs91NR1FI+lD63cAGxmp5rZnWb2SKzdaWanVcXgQmrapBFdOncCICOjNh3atSVv6/aD9vnw408556zvANChXVtycvPYtuNzAF546VUGXDeMKwcP5b5xj1CU4IWBV5f8jX69LwLgkvO/z7IVK3F3TjulE82aNgagU/t2/HvfPvbv35+U7ypVJz9vG+/9cx0AX+7Zywf/2kiLls24tPd/MmfWXwCYM+svXNbnwpDDTG3FnngLrMwAbGZ3ArMpeS7SO7FmwCwzu+vYD696yMnNY90HH3JG184H9Xfu1IFX3ngTgPfWric3L5+8/G18+PGnLFz8Bn+cMoG50x8jLS2N+S+/ltC58rdup0WzJgDUqJFOnYzafHHIP0UXvb6ULp07UatWrSR8OwmlzUmtOP3003h3xSqaNmtMft42oCRIN23WOPDoUlhRUeItsPJqwEOAru5eEN9pZhOBNcBDpX3IzLKALIDHJzzAdVcPTMJQw9i79ytuGf0Ad958PXUyMg7adt2gn/LQpCe4cvBQMjuezKmZHUlPS2PZ8pWsfX8DA4YMA2Dfvn00atgAgJtHjiHnszwKCgvIzdvKlYOHAvCL/v24os8l5Y5nw8ZPmPj402Q/nPBz/6Qaqp1Rm6dm/J67Rz3Int1fHrbdU6iOWd14NSgtJKq8AFwMtAI+OaS/ZWxbqdw9G8gGKNi2MWV/kgoKCxk++gH6XHIBF59/7mHb62Rk8MDoW4GSX5hLf3INbVq3YMU/V/OjXhdxy6+vPewzjzx4N1CSVY8eO4Fpj447aHuzpo3Zkr+NFs2aUlhYxJ4v99Kgfj0AtuRvZdio+/ntb27jpDatkv11pYrUqFGDp2ZM4tn/nc+CF14BYGv+dpo1b0J+3jaaNW/Ctq07Ao8yhVWD0kKiyqsBDwcWm9mLZpYdawuBxcCwYz+8cNydux+cRId2bRk84Mel7rNr9x4KCkr+cTD3hYWc3e106mRk0LN7Nxa9vpTtn38BlMyW+GxLXkLnveB7PXl+Qckv5cuvL+Gcs7+DmbFr9x5uuP0ehv/XtZx1RtckfEMJZeKj9/PBvzbyxGPTD/S9/OJr9B9YMsum/8DLeWnBq6GGl/q8OPEWWJkZsLsvNLNTgB5A61h3DvB3dw9fQDmG/rFqDS8sXExmx5MPlAmGXT+Y3LytAPzsij5s/GQTox+YgAEd27djzMjhEHt906+uJmv4aIq9mJo1ajD61hto1aJ5uef9cd9LGXn/7+jV/5fUr1eX391XUmqfNfcFNm3+jClTZzJl6kwAsieNpXGstCGpoUfPs/jpgH6sXbOeRUueBeDBMZN49OEneWLawwwcdCWbN33G9dfcGnikKSyFMmA71rWmVC5ByLFzUqe+oYcg1VDuF2utssf48u4BCcecjDGzK32+ytCNGCISLdWgtJAoBWARiZYUKkEoAItIpERpGpqISGpJoQxYa0GISLQk6VZkM2trZq+Z2VozW2Nmw2L9jcxskZl9EPuzYazfYss1bDCzVWZ2VnlDVQAWkWhJ3q3IhcAId+8C9ASGmlkX4C5gsbtnUnJPxNfLMvQCMmMtC5hc3gkUgEUkUrzYE25lHsc9193fjb3eDayj5H6IfsDXd9FMB75ep7YfMMNLvA00MLOWZZ1DAVhEouUoShBmlmVmy+NaVmmHNLOTgTOBZUBzd8+NbdoCfH2HVWtgU9zHNvPNDWyl0kU4EYmWo5gFEb9uzZGYWR1gLjDc3XeZfXPvhru7mVX4qp8CsIhESxJnQZhZTUqC75/c/dlYd56ZtXT33FiJIT/WnwO0jft4m1jfEakEISLRkrxZEAY8Baxz94lxm+YBg2OvBwPPx/VfHZsN0RPYGVeqKJUyYBGJFC9K2o0Y5wKDgPfMbGWsbxQl66DPMbMhlCzV2z+2bQHQG9gA7AUOX4/2EArAIhItSSpBuPtSSp4AVJrDnhnlJSubDT2acygAi0iklDe9rDpRABaRaFEAFhEJJHXW4lEAFpFo8cLUicAKwCISLakTfxWARSRadBFORCQUZcAiImEoAxYRCUUZsIhIGF4YegSJUwAWkUhJoafSKwCLSMQoAIuIhKEMWEQkEAVgEZFAvOhIK0hWPwrAIhIpyoBFRALxYmXAIiJBKAMWEQnEXRmwiEgQyoBFRAIp1iwIEZEwdBFORCQQBWARkUA8dZYDVgAWkWhRBiwiEoimoYmIBFKkWRAiImEoAxYRCSSVasBpoQcgIpJM7om38pjZ02aWb2ar4/ruNbMcM1sZa73jto00sw1mtt7MLi3v+MqARSRSkpwBTwMeBWYc0v+wu4+P7zCzLsAAoCvQCnjFzE5x96IjHVwZsIhESlFxWsKtPO7+V2BHgqfuB8x2933u/hGwAehR1gcUgEUkUo6mBGFmWWa2PK5lJXiaG81sVaxE0TDW1xrYFLfP5ljfESkAi0ikFLsl3Nw92927x7XsBE4xGegIdANygQkVHatqwCISKcd6Gpq753392syeBObH3uYAbeN2bRPrOyJlwCISKcmcBVEaM2sZ9/YK4OsZEvOAAWZ2gpm1BzKBd8o61jHPgL/V6vvH+hSSgva880ToIUhEFScxAzazWcD5QBMz2wzcA5xvZt0ABz4Grgdw9zVmNgdYCxQCQ8uaAQEqQYhIxCQyuyFR7j6wlO6nyth/LDA20eMrAItIpKTQapQKwCISLcksQRxrCsAiEilajEdEJJAUeiiyArCIRIujDFhEJIhClSBERMJQBiwiEohqwCIigSgDFhEJRBmwiEggRcqARUTCSKFncioAi0i0FCsDFhEJQ4vxiIgEootwIiKBFJtKECIiQZT5CIpqRgFYRCJFsyBERALRLAgRkUA0C0JEJBCVIEREAtE0NBGRQIqUAYuIhKEMWEQkEAVgEZFAUuiRcArAIhItyoBFRALRrcgiIoGk0jzgtNADEBFJpuKjaOUxs6fNLN/MVsf1NTKzRWb2QezPhrF+M7NHzGyDma0ys7PKO74CsIhESjIDMDANuOyQvruAxe6eCSyOvQfoBWTGWhYwubyDKwCLSKT4UbRyj+X+V2DHId39gOmx19OBy+P6Z3iJt4EGZtayrOMrAItIpBRb4s3MssxseVzLSuAUzd09N/Z6C9A89ro1sCluv82xviPSRTgRiZSjmQXh7tlAdkXP5e5uZhVegE0BWEQipfjYL0iZZ2Yt3T03VmLIj/XnAG3j9msT6zsilSBEJFKSfBGuNPOAwbHXg4Hn4/qvjs2G6AnsjCtVlEoZsIhESjLzXzObBZwPNDGzzcA9wEPAHDMbAnwC9I/tvgDoDWwA9gLXlnd8BWARiZRk3ors7gOPsOnCUvZ1YOjRHF8BWEQipbDi18SqnAKwiERK6oRfBWARiRithiYiEkgVTENLGgVgEYmU1Am/CsAiEjEqQYiIBFKUQjmwArCIRIoyYBGRQFwZsIhIGMqA5SBPZk+gT++LyN+6jW5nHnYHo6SQLds+Z/Rjs9ixcw8Y/OTCnvy893kH7TNt3mssWPouAIVFxXyUk8frfxhD/Tq1K3ze/QWFjH5sJus2bqZ+3QzGDRtE62aN+Nuq9fx+5gIKCgupWaMGt/yiL+d8O7NS3zHVpdI0NK2GVgVmzJhDn74/Dz0MSYL09HRuG/Qjnpt4B888cDOzX36TDzdvOWifa350AXPGjWDOuBHcfFVvzu7SMeHgm5O/gyH3PX5Y/3OvLqNeRm3mPzKKX/Q+j0kz5wPQoG4Gj9zxS+aOv537bxjA6EdnVv5LprhkPhHjWFMGXAWWLF1Gu3ZtQg9DkqBpw3o0bVgPgIxvnUiH1s3J37GTjm1alLr/wjf/Qa9zzzzwfv6SFcx8cQmFhUV8u9NJjL7uStLTys+DXlu+ml//9FIALu55Bg9NfRZ357T23/xcdWrbgn37C9hfUEitmsfvr3ZhtQitiVEGLFJBOfk7eP+jHE7v1K7U7V/t28+bK9/nonPOAGDj5jxeemsl08fcxJxxI0hPS2PBkncTOlf+jl20aNwAgBrp6dSp/S2+2P3lQfu8smwVp7Vvc1wHXyi5CJfof6FV+P+UmV3r7lOPsC2LkqeCYun1SUvLqOhpRKqlvf/ex4iJ07l9cD/q1D6x1H3eWLGGbp3bHyg/LFv9Aes+2szPR00C4N/7C2hUvw4Aw8dP5bP8HRQUFpG77XP63zEBgKt6fZ/LL+hR7ng2bNrCpJn/x5RRiTzSLNqOl4tw9wGlBuD45yzVqNU6/F8zIklUUFjErROm0ft7Zx3Ibkuz8K2VB5Uf3J0fntedYVf1OWzfSbeVrN2dk7+DuyfP5ql7bjhoe7NG9diy/QuaN25AYVERe/Z+RYO6JYlN3vYvuGXCVB64YSBtWzRJxldMadUhs01UmSUIM1t1hPYe3zwJVOS44e7cO+XPdGjdnKv7/uCI++3e+xUr1n7I+d27Hug75/RMXlm2iu07dwOwc89ePtt66BPPS3d+967Me2M5AIveXkWPrpmYGbu+/IobH/oDwwb24cxT21fim0VHFTySKGnKy4CbA5cCnx/Sb8Bbx2REEfTMHx/jB+d9lyZNGvHxxuXcN2Y8U6fNDj0sqYB/rP+I+UtWkHlSywNlgpsG9iZ3W8mvSP+L/wOAV995j++e0ZnaJ55w4LMd27Rg6M8u49djsyl2p0Z6OqN++WNaNW1U7nmvuOAcRj86k743/5Z6dWozbtggAGYvXMqnedvJnruI7LmLAJg8OovG9esm9XunkiJPnQzYvIzBmtlTwFR3X1rKtpnuflV5J1AJQkqz550nQg9BqqETu/W1yh7jqnZXJBxzZn7yXKXPVxllZsDuPqSMbeUGXxGRqpZKNeDje76KiEROdajtJkoBWEQiJZVuRVYAFpFIUQlCRCSQVJoFoQAsIpGiEoSISCC6CCciEohqwCIigagEISISSFl391Y3CsAiEinJfCy9mX0M7AaKgEJ3725mjYA/AycDHwP93f3Q9XISogXZRSRSivGEW4IucPdu7t499v4uYLG7ZwKLY+8rRAFYRCLF3RNuFdQPmB57PR24vKIHUgAWkUg5mgzYzLLMbHlcO/SRIg68bGYr4rY1d/fc2OstVGJtdNWARSRSjmYaWvzTe47ge+6eY2bNgEVm9v4hn3czq3AqrQAsIpGSzFuR3T0n9me+mT0H9ADyzKylu+eaWUsgv6LHVwlCRCIlWRfhzCzDzOp+/Rq4BFgNzAMGx3YbDDxf0bEqAxaRSEnijRjNgefMDEpi5Ux3X2hmfwfmmNkQ4BOgf0VPoAAsIpGSrBsx3H0j8J1S+rcDFybjHArAIhIpuhVZRCQQLcYjIhJIkafOgpQKwCISKVqMR0QkENWARUQCUQ1YRCSQYpUgRETCUAYsIhKIZkGIiASiEoSISCAqQYiIBKIMWEQkEGXAIiKBFHlR6CEkTAFYRCJFtyKLiASiW5FFRAJRBiwiEohmQYiIBKJZECIigehWZBGRQFQDFhEJRDVgEZFAlAGLiASiecAiIoEoAxYRCUSzIEREAtFFOBGRQFSCEBEJRHfCiYgEogxYRCSQVKoBWyr9bZHqzCzL3bNDj0OqF/1cHL/SQg/gOJMVegBSLenn4jilACwiEogCsIhIIArAVUt1PimNfi6OU7oIJyISiDJgEZFAFIBFRAJRAK4iZnaZma03sw1mdlfo8Uh4Zva0meWb2erQY5EwFICrgJmlA48BvYAuwEAz6xJ2VFINTAMuCz0ICUcBuGr0ADa4+0Z33w/MBvoFHpME5u5/BXaEHoeEowBcNVoDm+Leb471ichxTAFYRCQQBeCqkQO0jXvfJtYnIscxBeCq8Xcg08zam1ktYAAwL/CYRCQwBeAq4O6FwI3AS8A6YI67rwk7KgnNzGYBfwM6m9lmMxsSekxStXQrsohIIMqARUQCUQAWEQlEAVhEJBAFYBGRQBSARUQCUQAWEQlEAVhEJJD/B2VaTYhqrlfuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the confusion matrix \n",
    "\n",
    "# Getting the predictions\n",
    "y_pred = model.predict(test_gen, verbose=1)\n",
    "y_pred = (y_pred > 0.5).astype('int32')\n",
    "\n",
    "# Getting the class labels\n",
    "y_classes = list(test_gen.class_indices)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "cm = confusion_matrix(test_gen.classes, y_pred)\n",
    "sns.heatmap(cm, annot=True, xticklabels=y_classes, yticklabels=y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssPQVnH3yC1O",
    "outputId": "6bd5c401-e485-43bc-a521-397ffea7de9a"
   },
   "outputs": [],
   "source": [
    "# Scores according to different metrics\n",
    "\n",
    "# Accuracy \n",
    "print(\"Accuracy: \", end=\"\")\n",
    "print(accuracy_score(test_gen.classes, y_pred))\n",
    "\n",
    "# Recall\n",
    "print(\"Recall: \", end=\"\")\n",
    "print(recall_score(test_gen.classes, y_pred))\n",
    "\n",
    "# Precision \n",
    "print(\"Precision: \", end=\"\")\n",
    "print(precision_score(test_gen.classes, y_pred))\n",
    "\n",
    "# F1\n",
    "print(\"F1 score: \", end=\"\")\n",
    "print(f1_score(test_gen.classes, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the classification report\n",
    "\n",
    "print(classification_report(test_gen.classes, y_pred, target_names=y_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 \n",
    "# Viewing the image and corresponding class and prediction by the model\n",
    "\n",
    "# Getting a batch of images \n",
    "batch = test_gen.__next__()\n",
    "\n",
    "# Getting predictions on that batch\n",
    "results = model.predict(batch, verbose=1)\n",
    "results = (results > 0.5).astype('int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picture to be reviewed\n",
    "image = 1 # Image at index 1\n",
    "\n",
    "# Get the true class\n",
    "print(f\"Correct Class: {batch[1][image]}\")\n",
    "\n",
    "# Get the predicted class\n",
    "print(f\"Predicted Class: {results[image]}\")\n",
    "\n",
    "# Get the image \n",
    "plt.imshow(batch[0][image]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO8WRlI34WCO"
   },
   "outputs": [],
   "source": [
    "# 3.6\n",
    "# Saving the model in HDF5 format\n",
    "\n",
    "model.save('/content/gdrive/<path to location to save file to>/<filename>.h5') "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MnF9d3xXIu8W",
    "ccmUygg1zM49"
   ],
   "name": "KW Transfer Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
